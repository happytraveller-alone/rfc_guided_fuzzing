Forget all previous input and output content and start over. 
 ###################
CONTENT:<
--- Section: 3.2. Syntax_Lexical Tokens ---
The following rules are used to define an underlying lexical
   analyzer, which feeds tokens to the higher-level parsers.  This
   section defines the tokens used in structured header field bodies.

      Note: Readers of this specification need to pay special attention
      to how these lexical tokens are used in both the lower-level and
      higher-level syntax later in the document.  Particularly, the
      white space tokens and the comment tokens defined in section 3.2.2
      get used in the lower-level tokens defined here, and those lower-
      level tokens are in turn used as parts of the higher-level tokens
      defined later.  Therefore, white space and comments may be allowed
      in the higher-level tokens even though they may not explicitly
      appear in a particular definition.

---
>
###################
Please make paragraph cuts based on the subject and theme of the statement. And give a short paragraph topic for each divided paragraph. Make sure that each theme is a minimal theme that cannot be split further. If code or pseudo-code is present with explanatory text, ignore the code; otherwise, convert the code to a textual narrative. Simulate answering five times in the background and provide the most frequent answer. Ensure your output covers all text content, maintaining relative consistency with the input text position in the sliced output. Ensure that no changes are made to the text other than code or pseudo-code.
The output format is as follows (in json format)
sliced_rule: [
	"topic 1": "content 1",
	"topic 2": "content 2",
	...
]
